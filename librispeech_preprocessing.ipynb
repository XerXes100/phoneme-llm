{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KibCCdC3JB9M",
        "outputId": "b90f0e5b-8d5a-42b0-b630-1f22bea397d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=0ce239eecfbf90fc4694cfec7b3bd948b23909305dac6c83420306540747457c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "PheNr3ZVJeTx"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://us.openslr.org/resources/12/train-clean-100.tar.gz'  # Replace with your dataset URL\n",
        "\n",
        "# # Define a custom progress bar callback for tqdm\n",
        "# def bar_progress(current, total, width=80):\n",
        "#     progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "#     return progress_message\n",
        "\n",
        "# # Download the dataset file to the current directory in Colab with tqdm progress bar\n",
        "# output_file = wget.download(url, bar=bar_progress)\n",
        "# print(f\"\\n{output_file} Download success!\")\n"
      ],
      "metadata": {
        "id": "DgSFNHYp-1ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://us.openslr.org/resources/12/train-clean-360.tar.gz'  # Replace with your dataset URL\n",
        "\n",
        "# # Define a custom progress bar callback for tqdm\n",
        "# def bar_progress(current, total, width=80):\n",
        "#     progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "#     return progress_message\n",
        "\n",
        "# # Download the dataset file to the current directory in Colab with tqdm progress bar\n",
        "# output_file = wget.download(url, bar=bar_progress)\n",
        "# print(f\"\\n{output_file} Download success!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUJA_TfeL98K",
        "outputId": "80a6d6c8-f8f5-4dec-cea5-94145560e0a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 34% [7931731968 / 23049477885] bytesBuffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://us.openslr.org/resources/12/train-other-500.tar.gz'  # Replace with your dataset URL\n",
        "\n",
        "# # Define a custom progress bar callback for tqdm\n",
        "# def bar_progress(current, total, width=80):\n",
        "#     progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "#     return progress_message\n",
        "\n",
        "# # Download the dataset file to the current directory in Colab with tqdm progress bar\n",
        "# output_file = wget.download(url, bar=bar_progress)\n",
        "# print(f\"\\n{output_file} Download success!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQnn2jY1MPVG",
        "outputId": "c558abb5-702f-4669-e8ca-eaf424ba877e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 25% [7951736832 / 30593501606] bytesBuffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -xzf /content/train-clean-100.tar.gz\n",
        "# !tar -xzf /content/train-clean-360.tar.gz\n",
        "# !tar -xzf /content/train-other-500.tar.gz"
      ],
      "metadata": {
        "id": "FITmcxlPVsXG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the root directory containing the LibriSpeech data\n",
        "root_dir = '/content/LibriSpeech'\n",
        "\n",
        "# Create an empty list to store text file paths\n",
        "text_files = []\n",
        "\n",
        "# Recursive function to collect paths of all text files\n",
        "def collect_text_files(directory):\n",
        "  _, dirnames, _ = next(os.walk(directory))\n",
        "\n",
        "  for dir in dirnames:\n",
        "      curr_dir = os.path.join(directory, dir)\n",
        "      _, in_dirnames, _ = next(os.walk(curr_dir))\n",
        "\n",
        "      for in_dir in in_dirnames:\n",
        "        curr_dir = os.path.join(directory, dir, in_dir)\n",
        "        in_in_tuple = next(os.walk(curr_dir))\n",
        "        flac_txt_files = in_in_tuple[-1]\n",
        "\n",
        "        for i_i_f in flac_txt_files:\n",
        "          if i_i_f.endswith('.txt'):\n",
        "            text_files.append(os.path.join(directory, dir, in_dir, i_i_f))\n",
        "\n",
        "\n",
        "dataset = 'train-other-500'\n",
        "# Collect paths of all text files in the LibriSpeech directory\n",
        "collect_text_files(os.path.join(root_dir, dataset))\n",
        "\n",
        "# Concatenate contents of all text files into a single larger text file\n",
        "output_text = f'{dataset}.txt'\n",
        "\n",
        "with open(output_text, 'w') as outfile:\n",
        "    for text_file in text_files:\n",
        "        with open(text_file, 'r') as infile:\n",
        "            outfile.write(infile.read())\n",
        "            # outfile.write('\\n')  # Add a newline between text files\n",
        "\n",
        "print(f\"Concatenated text saved to '{output_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4-mXKpS6o40",
        "outputId": "9ce71ee8-7003-4373-f844-404b8489ba08"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated text saved to 'train-other-500.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file names to be concatenated\n",
        "file_names = ['train-clean-100.txt', 'train-clean-360.txt', 'train-other-500.txt']  # Replace with your file names\n",
        "\n",
        "# Name of the output concatenated file\n",
        "output_file = 'train-960.txt'  # Replace with your desired output file name\n",
        "\n",
        "# Open output file in append mode to concatenate the content\n",
        "with open(output_file, 'a') as outfile:\n",
        "    for file_name in file_names:\n",
        "        # Open each input file and read its content\n",
        "        with open(file_name, 'r') as infile:\n",
        "            content = infile.read()\n",
        "\n",
        "        # Write the content of the input file into the output file\n",
        "        outfile.write(content)\n",
        "\n",
        "print(f\"Concatenated files {file_names} into '{output_file}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07xG6EyyDW6i",
        "outputId": "99a2bb57-d176-4219-f323-148a102af092"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated files ['train-clean-100.txt', 'train-clean-360.txt', 'train-other-500.txt'] into 'train-960.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'train-960.txt'  # Replace with your input file name\n",
        "output_file = 'final-train-960.txt'  # Replace with your desired output file name\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        _, sep, remainder = line.partition(' ')\n",
        "        updated_line = remainder.strip()  # Preserve leading whitespace if needed\n",
        "        outfile.write(updated_line + '\\n')\n",
        "\n",
        "print(f\"Updated file written to '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD4nmE7ZH-m-",
        "outputId": "bf2a7894-3180-44a1-d0cc-f7e102811571"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file written to 'final-train-960.txt'\n"
          ]
        }
      ]
    }
  ]
}